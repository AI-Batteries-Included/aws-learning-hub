# Amazon S3 Storage

Master Amazon S3 object storage - the foundation of most AWS architectures. Learn how to store, secure, and manage data at scale with hands-on examples and best practices.

## What is Amazon S3?

Amazon Simple Storage Service (S3) is object storage built to store and retrieve any amount of data from anywhere. It's designed to deliver 99.999999999% (11 9's) durability and scales to trillions of objects worldwide.

### Key Benefits
- **Virtually unlimited capacity** - Store as much data as you need
- **High durability** - 99.999999999% (11 9's) data durability  
- **High availability** - 99.99% availability SLA
- **Cost-effective** - Multiple storage classes for different use cases
- **Security** - Comprehensive security and compliance capabilities
- **Performance** - Low latency and high throughput

## Core Concepts

### Buckets
Containers that hold your objects (files). Think of them as top-level folders.

**Key Properties:**
- **Globally unique names** - Must be unique across all AWS accounts
- **Regional** - Created in a specific AWS region
- **Unlimited objects** - No limit on number of objects stored
- **Flat structure** - No true folder hierarchy (simulated with prefixes)

```bash
# Create a bucket
aws s3 mb s3://my-unique-bucket-name-2024

# List buckets
aws s3 ls
```

### Objects
Individual files stored in buckets. Can be any type of file up to 5TB.

**Object Components:**
- **Key** - Object name (including "path")
- **Value** - Object data (the file content)
- **Metadata** - Key-value pairs describing the object
- **Version ID** - For versioned objects
- **Access Control** - Permissions for the object

```bash
# Upload object
aws s3 cp file.txt s3://my-bucket/folder/file.txt

# Download object
aws s3 cp s3://my-bucket/folder/file.txt ./downloaded-file.txt

# List objects
aws s3 ls s3://my-bucket/ --recursive
```

### Storage Classes
Different storage options optimized for different use cases and costs.

| Storage Class | Use Case | Retrieval Time | Cost |
|---------------|----------|----------------|------|
| **Standard** | Frequently accessed data | Immediate | Higher |
| **Intelligent-Tiering** | Unknown/changing access patterns | Immediate | Automatic optimization |
| **Standard-IA** | Infrequently accessed data | Immediate | Lower storage, retrieval fee |
| **One Zone-IA** | Non-critical, infrequent access | Immediate | Lowest immediate access |
| **Glacier Instant** | Archive with immediate access | Immediate | Low storage cost |
| **Glacier Flexible** | Archive data | 1-12 hours | Very low cost |
| **Glacier Deep Archive** | Long-term archive | 12-48 hours | Lowest cost |

## Getting Started

### 1. Create Your First Bucket

**Using AWS Console:**
1. Navigate to S3 console
2. Click "Create bucket"
3. Choose unique name and region
4. Configure settings
5. Click "Create bucket"

**Using AWS CLI:**
```bash
# Create bucket
aws s3 mb s3://my-learning-bucket-$(date +%s)

# Verify creation
aws s3 ls | grep my-learning-bucket
```

**Using CloudFormation:**
```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Basic S3 bucket'

Resources:
  MyS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-learning-bucket'
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

Outputs:
  BucketName:
    Description: 'Name of the S3 bucket'
    Value: !Ref MyS3Bucket
    Export:
      Name: !Sub '${AWS::StackName}-BucketName'
```

### 2. Upload Your First Object

```bash
# Create a test file
echo "Hello, S3!" > hello.txt

# Upload to bucket
aws s3 cp hello.txt s3://my-learning-bucket/hello.txt

# Verify upload
aws s3 ls s3://my-learning-bucket/
```

### 3. Set Up Basic Security

```bash
# Block all public access (recommended default)
aws s3api put-public-access-block \
  --bucket my-learning-bucket \
  --public-access-block-configuration \
  BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true
```

## Common Use Cases

### 1. Static Website Hosting

Host static websites directly from S3.

```bash
# Enable static website hosting
aws s3api put-bucket-website \
  --bucket my-website-bucket \
  --website-configuration '{
    "IndexDocument": {"Suffix": "index.html"},
    "ErrorDocument": {"Key": "error.html"}
  }'

# Upload website files
aws s3 sync ./website-files s3://my-website-bucket/
```

### 2. Data Backup and Archival

```bash
# Backup with lifecycle policy
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-backup-bucket \
  --lifecycle-configuration '{
    "Rules": [{
      "ID": "BackupRule",
      "Status": "Enabled",
      "Filter": {"Prefix": "backups/"},
      "Transitions": [
        {"Days": 30, "StorageClass": "STANDARD_IA"},
        {"Days": 90, "StorageClass": "GLACIER"},
        {"Days": 365, "StorageClass": "DEEP_ARCHIVE"}
      ]
    }]
  }'
```

### 3. Content Distribution

Use with CloudFront for global content delivery.

```yaml
# CloudFormation template
Resources:
  ContentBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-content'
      
  CloudFrontDistribution:
    Type: AWS::CloudFront::Distribution
    Properties:
      DistributionConfig:
        Origins:
          - Id: S3Origin
            DomainName: !GetAtt ContentBucket.DomainName
            S3OriginConfig:
              OriginAccessIdentity: !Sub 'origin-access-identity/cloudfront/${OriginAccessIdentity}'
        DefaultCacheBehavior:
          TargetOriginId: S3Origin
          ViewerProtocolPolicy: redirect-to-https
```

## Best Practices

### Security
- ✅ **Block public access** by default
- ✅ **Enable encryption** at rest and in transit
- ✅ **Use IAM policies** instead of ACLs
- ✅ **Enable MFA Delete** for critical buckets
- ✅ **Regular access reviews** and cleanup
- ✅ **Use VPC endpoints** for private access

### Performance  
- ✅ **Use random key prefixes** for high request rates
- ✅ **Implement multipart upload** for large files
- ✅ **Use Transfer Acceleration** for global uploads
- ✅ **Cache frequently accessed objects**
- ✅ **Monitor request patterns** and optimize

### Cost Management
- ✅ **Use appropriate storage classes**
- ✅ **Implement lifecycle policies**
- ✅ **Clean up incomplete uploads**
- ✅ **Monitor usage with Cost Explorer**
- ✅ **Use S3 Analytics** for access patterns

### Operations
- ✅ **Enable versioning** for important data
- ✅ **Set up cross-region replication** for DR
- ✅ **Configure event notifications** for automation
- ✅ **Regular backups** and disaster recovery testing
- ✅ **Monitor with CloudWatch** and set up alerts

## Real-World Example: Photo Storage App

Let's build a complete photo storage solution:

```yaml
AWSTemplateFormatVersion: '2010-09-09'

Resources:
  PhotoBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-photos'
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders: ['*']
            AllowedMethods: [GET, PUT, POST, DELETE, HEAD]
            AllowedOrigins: ['*']
            MaxAge: 3000
      LifecycleConfiguration:
        Rules:
          - Id: OptimizePhotos
            Status: Enabled
            Transitions:
              - Days: 30
                StorageClass: STANDARD_IA
              - Days: 365
                StorageClass: GLACIER
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt ProcessPhotoFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .jpg

  ProcessPhotoFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.9
      Handler: index.handler
      Code:
        ZipFile: |
          import json
          import boto3
          
          def handler(event, context):
              s3 = boto3.client('s3')
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  print(f'Processing photo: {key} in bucket: {bucket}')
                  
                  # Add your photo processing logic here
                  # e.g., create thumbnails, extract metadata, etc.
                  
              return {'statusCode': 200}
```

This comprehensive S3 guide provides everything you need to master Amazon S3 storage, from basic concepts to advanced enterprise patterns. Practice with the examples and gradually build more complex solutions!